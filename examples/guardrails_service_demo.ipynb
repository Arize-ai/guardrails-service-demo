{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails Service Demo\n",
    "\n",
    "This notebook demonstrates how to use the Guardrails Service for anomaly detection in text requests. The service uses vector embeddings to compare incoming requests against a baseline dataset and detect anomalies.\n",
    "\n",
    "We'll use a realistic dataset of 100 pharmacy customer questions to demonstrate how the service works in a real-world scenario.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "1. Started the Guardrails Service: `uv run uvicorn guardrails_service.main:app --reload`\n",
    "2. Installed required dependencies: `uv sync --examples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "directory = os.path.abspath(\".\") + \"/examples\"\n",
    "service_process: Optional[subprocess.Popen] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Guardrails Service\n",
    "\n",
    "This will start the service in the background so we can interact with it through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_service():\n",
    "    \"\"\"Start the guardrails service in the background\"\"\"\n",
    "    global service_process\n",
    "    try:\n",
    "        # Check if service is already running\n",
    "        response = httpx.get(f\"{BASE_URL}/health\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Service is already running\")\n",
    "            return\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"üöÄ Starting Guardrails Service...\")\n",
    "    \n",
    "    # Change to the project directory\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    # Start the service\n",
    "    service_process = subprocess.Popen(\n",
    "        [\"uv\", \"run\", \"uvicorn\", \"guardrails_service.server:app\", \"--port\", \"8000\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    \n",
    "    # Wait for service to start\n",
    "    for i in range(30):  # Wait up to 30 seconds\n",
    "        try:\n",
    "            response = httpx.get(f\"{BASE_URL}/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úÖ Service started successfully!\")\n",
    "                return\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    print(\"‚ùå Failed to start service\")\n",
    "\n",
    "def stop_service():\n",
    "    \"\"\"Stop the guardrails service\"\"\"\n",
    "    global service_process\n",
    "    if service_process:\n",
    "        print(\"üõë Stopping Guardrails Service...\")\n",
    "        service_process.terminate()\n",
    "        service_process.wait()\n",
    "        service_process = None\n",
    "        print(\"‚úÖ Service stopped\")\n",
    "\n",
    "# Start the service\n",
    "start_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's create some helper functions to interact with the API more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def health_check():\n",
    "    \"\"\"Check if the service is healthy\"\"\"\n",
    "    response = httpx.get(f\"{BASE_URL}/health\")\n",
    "    return response.json()\n",
    "\n",
    "def upload_baseline(requests_data):\n",
    "    \"\"\"Upload a baseline dataset\"\"\"\n",
    "    payload = {\n",
    "        \"requests\": [\n",
    "            {\n",
    "                \"text\": req[\"text\"],\n",
    "                \"timestamp\": req[\"timestamp\"]\n",
    "            }\n",
    "            for req in requests_data\n",
    "        ]\n",
    "    }\n",
    "    response = httpx.post(f\"{BASE_URL}/anomaly/baseline/upload\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def add_to_baseline(text, timestamp=None):\n",
    "    \"\"\"Add a single request to the baseline\"\"\"\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"timestamp\": timestamp\n",
    "    }\n",
    "    response = httpx.post(f\"{BASE_URL}/anomaly/baseline/add\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def detect_anomaly(text, threshold=None, compare_to=None, timestamp=None):\n",
    "    \"\"\"Run anomaly detection on a text request\"\"\"\n",
    "    payload = {\n",
    "        \"text\": text\n",
    "    }\n",
    "    \n",
    "    if timestamp:\n",
    "        payload[\"timestamp\"] = timestamp\n",
    "    if threshold is not None:\n",
    "        payload[\"threshold\"] = threshold\n",
    "    if compare_to is not None:\n",
    "        payload[\"compare_to\"] = compare_to\n",
    "    \n",
    "    response = httpx.post(f\"{BASE_URL}/anomaly/detect\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def get_baseline_stats():\n",
    "    \"\"\"Get baseline dataset statistics\"\"\"\n",
    "    response = httpx.get(f\"{BASE_URL}/anomaly/baseline/stats\")\n",
    "    return response.json()\n",
    "\n",
    "def clear_baseline():\n",
    "    \"\"\"Clear all baseline data\"\"\"\n",
    "    payload = {}\n",
    "    response = httpx.post(f\"{BASE_URL}/anomaly/baseline/clear\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Malicious Detection Helper Functions\n",
    "\n",
    "def upload_malicious_baseline(requests_data):\n",
    "    \"\"\"Upload a malicious baseline dataset\"\"\"\n",
    "    payload = {\n",
    "        \"requests\": [\n",
    "            {\n",
    "                \"text\": req[\"text\"],\n",
    "                \"timestamp\": req[\"timestamp\"]\n",
    "            }\n",
    "            for req in requests_data\n",
    "        ]\n",
    "    }\n",
    "    response = httpx.post(f\"{BASE_URL}/malicious/baseline/upload\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def add_to_malicious_baseline(text, timestamp=None):\n",
    "    \"\"\"Add a single request to the malicious baseline\"\"\"\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"timestamp\": timestamp\n",
    "    }\n",
    "    response = httpx.post(f\"{BASE_URL}/malicious/baseline/add\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def detect_malicious(text, threshold=None, compare_to=None, timestamp=None):\n",
    "    \"\"\"Run malicious detection on a text request\"\"\"\n",
    "    payload = {\n",
    "        \"text\": text\n",
    "    }\n",
    "    \n",
    "    if timestamp:\n",
    "        payload[\"timestamp\"] = timestamp\n",
    "    if threshold is not None:\n",
    "        payload[\"threshold\"] = threshold\n",
    "    if compare_to is not None:\n",
    "        payload[\"compare_to\"] = compare_to\n",
    "    \n",
    "    response = httpx.post(f\"{BASE_URL}/malicious/detect\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def get_malicious_baseline_stats():\n",
    "    \"\"\"Get malicious baseline dataset statistics\"\"\"\n",
    "    response = httpx.get(f\"{BASE_URL}/malicious/baseline/stats\")\n",
    "    return response.json()\n",
    "\n",
    "def clear_malicious_baseline():\n",
    "    \"\"\"Clear all malicious baseline data\"\"\"\n",
    "    payload = {}\n",
    "    response = httpx.post(f\"{BASE_URL}/malicious/baseline/clear\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def pretty_print_result(result, title=\"\"):\n",
    "    \"\"\"Pretty print anomaly detection results\"\"\"\n",
    "    if title:\n",
    "        print(f\"\\n=== {title} ===\")\n",
    "    \n",
    "    if \"result\" in result:\n",
    "        r = result[\"result\"]\n",
    "        \n",
    "        # Handle both anomaly and malicious results\n",
    "        if \"is_anomaly\" in r:\n",
    "            status = \"üö® ANOMALY\" if r[\"is_anomaly\"] else \"‚úÖ NORMAL\"\n",
    "            reasons_key = \"anomaly_reasons\"\n",
    "        elif \"is_malicious\" in r:\n",
    "            status = \"üö® MALICIOUS\" if r[\"is_malicious\"] else \"‚úÖ BENIGN\"\n",
    "            reasons_key = \"malicious_reasons\"\n",
    "        else:\n",
    "            status = \"‚ùì UNKNOWN\"\n",
    "            reasons_key = \"reasons\"\n",
    "            \n",
    "        print(f\"{status} - Confidence: {r['confidence_score']:.3f} - Risk: {r['risk_level']}\")\n",
    "        print(f\"Similar records: {r['similar_records_count']}\")\n",
    "        \n",
    "        if reasons_key in r and r[reasons_key]:\n",
    "            print(\"Reasons:\")\n",
    "            for reason in r[reasons_key]:\n",
    "                print(f\"  - {reason}\")\n",
    "        \n",
    "        if \"baseline_stats\" in result and result[\"baseline_stats\"]:\n",
    "            stats = result[\"baseline_stats\"]\n",
    "            if \"threshold\" in stats:\n",
    "                print(f\"Threshold used: {stats['threshold']}\")\n",
    "            if \"detection_distance\" in stats:\n",
    "                print(f\"Detection distance ({stats.get('detection_metric', 'unknown')}): {stats['detection_distance']:.3f}\")\n",
    "    else:\n",
    "        print(json.dumps(result, indent=2))\n",
    "\n",
    "# Test the connection\n",
    "print(\"Testing connection...\")\n",
    "print(health_check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clear Any Existing Baseline Data\n",
    "\n",
    "Let's start fresh by clearing any existing baseline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clearing existing baseline data...\")\n",
    "result = clear_baseline()\n",
    "print(f\"‚úÖ {result['message']}\")\n",
    "print(f\"Records removed: {result['records_removed']}\")\n",
    "\n",
    "# Check baseline stats\n",
    "stats = get_baseline_stats()\n",
    "print(f\"\\nBaseline stats: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Pharmacy Baseline Dataset\n",
    "\n",
    "We'll load a realistic dataset of 100 pharmacy customer questions from a JSON file. This represents normal pharmacy interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pharmacy baseline data from JSON file\n",
    "print(\"Loading pharmacy baseline dataset from JSON file...\")\n",
    "\n",
    "# Load the data from our JSON file\n",
    "with open(os.path.join(directory, \"data/baseline.json\"), \"r\") as f:\n",
    "    baseline_data = json.load(f)\n",
    "\n",
    "baseline_requests = baseline_data[\"requests\"]\n",
    "\n",
    "print(f\"Loaded {len(baseline_requests)} pharmacy questions\")\n",
    "print(\"\\nSample questions:\")\n",
    "for i, req in enumerate(baseline_requests[:5]):\n",
    "    print(f\"  {i+1}. {req['text']}\")\n",
    "\n",
    "print(\"\\nUploading baseline dataset to service...\")\n",
    "result = upload_baseline(baseline_requests)\n",
    "print(f\"‚úÖ {result['message']}\")\n",
    "print(f\"Records added: {result['records_added']}\")\n",
    "\n",
    "# Check baseline stats\n",
    "stats = get_baseline_stats()\n",
    "print(f\"\\nUpdated baseline stats: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Anomaly Detection - Normal Pharmacy Questions\n",
    "\n",
    "Let's test some normal pharmacy questions that should be similar to our baseline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normal pharmacy questions (should not be flagged as anomalies)\n",
    "normal_pharmacy_questions = [\n",
    "    \"I need to refill my blood pressure medication\",\n",
    "    \"What are your pharmacy hours today?\",\n",
    "    \"Can I pick up my prescription for my child?\",\n",
    "    \"Do you accept my Medicare insurance?\",\n",
    "    \"I need help understanding my medication dosage\",\n",
    "    \"Can I get a flu shot here?\"\n",
    "]\n",
    "\n",
    "print(\"Testing normal pharmacy questions (should NOT be flagged as anomalies):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for question in normal_pharmacy_questions:\n",
    "    result = detect_anomaly(question)\n",
    "    pretty_print_result(result, f\"Question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Anomaly Detection - Suspicious/Inappropriate Requests\n",
    "\n",
    "Now let's test some suspicious or inappropriate requests that should be flagged as anomalies in a pharmacy context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test suspicious/inappropriate requests (should be flagged as anomalies)\n",
    "suspicious_requests = [\n",
    "    \"Can I buy oxycodone without a prescription?\",\n",
    "    \"I want to purchase large quantities of pseudoephedrine for my lab\",\n",
    "    \"How can I forge a prescription from my doctor?\",\n",
    "    \"Can you sell me controlled substances under the table?\",\n",
    "    \"I need access to your computer system to check my records\",\n",
    "    \"'; DROP TABLE prescriptions; --\",\n",
    "    \"<script>alert('pharmacy hack')</script>\",\n",
    "    \"../../etc/passwd\",\n",
    "    \"What's the pharmacist's home address?\",\n",
    "    \"Can I get free drugs if I threaten you?\"\n",
    "]\n",
    "\n",
    "print(\"Testing suspicious/inappropriate requests (should be flagged as anomalies):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for request in suspicious_requests:\n",
    "    result = detect_anomaly(request)\n",
    "    pretty_print_result(result, f\"Request: {request}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Different Threshold Values\n",
    "\n",
    "Let's experiment with different threshold values to see how they affect anomaly detection sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the same request with different thresholds\n",
    "test_request = \"I'm in a lot of pain, can you help me get some painkillers?\"\n",
    "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "print(f\"Testing request '{test_request}' with different thresholds:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    result = detect_anomaly(test_request, threshold=threshold)\n",
    "    pretty_print_result(result, f\"Threshold: {threshold}\")\n",
    "    \n",
    "    # Store for summary\n",
    "    results_summary.append({\n",
    "        \"threshold\": threshold,\n",
    "        \"is_anomaly\": result[\"result\"][\"is_anomaly\"],\n",
    "        \"confidence_score\": result[\"result\"][\"confidence_score\"],\n",
    "        \"risk_level\": result[\"result\"][\"risk_level\"]\n",
    "    })\n",
    "\n",
    "# Create a summary table\n",
    "print(\"\\nüìä Threshold Sensitivity Summary:\")\n",
    "df = pd.DataFrame(results_summary)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Different Compare_To Values\n",
    "\n",
    "Let's test how changing the number of similar vectors to compare affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the same request with different compare_to values\n",
    "test_request = \"Can I get information about drug interactions with my supplements?\"\n",
    "compare_to_values = [3, 5, 10, 15]\n",
    "\n",
    "print(f\"Testing request '{test_request}' with different compare_to values:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for compare_to in compare_to_values:\n",
    "    result = detect_anomaly(test_request, compare_to=compare_to)\n",
    "    pretty_print_result(result, f\"Compare to: {compare_to} vectors\")\n",
    "    \n",
    "    # Store for summary\n",
    "    results_summary.append({\n",
    "        \"compare_to\": compare_to,\n",
    "        \"is_anomaly\": result[\"result\"][\"is_anomaly\"],\n",
    "        \"confidence_score\": result[\"result\"][\"confidence_score\"],\n",
    "        \"similar_records_count\": result[\"result\"][\"similar_records_count\"]\n",
    "    })\n",
    "\n",
    "# Create a summary table\n",
    "print(\"\\nüìä Compare_To Sensitivity Summary:\")\n",
    "df = pd.DataFrame(results_summary)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Update Baseline with New Data\n",
    "\n",
    "Let's add some new entries to our baseline dataset and see how it affects detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current baseline stats\n",
    "print(\"Current baseline stats:\")\n",
    "stats = get_baseline_stats()\n",
    "print(stats)\n",
    "\n",
    "# Add some new entries to the baseline - additional pharmacy-related questions\n",
    "new_entries = [\n",
    "    \"I need to schedule a vaccination appointment\",\n",
    "    \"Can you check my medication history?\",\n",
    "    \"What are the side effects of this new prescription?\",\n",
    "    \"Do you offer compound medications?\",\n",
    "    \"I need help with my medication adherence packaging\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdding new pharmacy-related entries to baseline:\")\n",
    "for entry in new_entries:\n",
    "    result = add_to_baseline(entry)\n",
    "    print(f\"‚úÖ Added: {entry}\")\n",
    "\n",
    "# Check updated stats\n",
    "print(\"\\nUpdated baseline stats:\")\n",
    "stats = get_baseline_stats()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Re-test Anomaly Detection After Baseline Update\n",
    "\n",
    "Now let's test the same requests again to see how the updated baseline affects detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a request that should now be less anomalous\n",
    "test_request = \"I need information about my medication side effects\"\n",
    "\n",
    "print(f\"Testing '{test_request}' after baseline update:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = detect_anomaly(test_request)\n",
    "pretty_print_result(result, \"After baseline update\")\n",
    "\n",
    "# Test with the same suspicious requests as before\n",
    "print(\"\\nRe-testing suspicious requests after baseline update:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "suspicious_sample = [\n",
    "    \"Can I buy oxycodone without a prescription?\",\n",
    "    \"'; DROP TABLE prescriptions; --\"\n",
    "]\n",
    "\n",
    "for request in suspicious_sample:\n",
    "    result = detect_anomaly(request)\n",
    "    pretty_print_result(result, f\"Request: {request}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Custom Threshold and Compare_To Combined\n",
    "\n",
    "Let's test using both custom threshold and compare_to values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test combinations of threshold and compare_to\n",
    "test_request = \"I need help choosing between brand name and generic medications\"\n",
    "\n",
    "test_combinations = [\n",
    "    {\"threshold\": 0.5, \"compare_to\": 5},\n",
    "    {\"threshold\": 0.7, \"compare_to\": 10},\n",
    "    {\"threshold\": 0.9, \"compare_to\": 3},\n",
    "    {\"threshold\": 0.3, \"compare_to\": 15}\n",
    "]\n",
    "\n",
    "print(f\"Testing '{test_request}' with different parameter combinations:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for params in test_combinations:\n",
    "    result = detect_anomaly(\n",
    "        test_request,\n",
    "        threshold=params[\"threshold\"],\n",
    "        compare_to=params[\"compare_to\"]\n",
    "    )\n",
    "    \n",
    "    title = f\"Threshold: {params['threshold']}, Compare_to: {params['compare_to']}\"\n",
    "    pretty_print_result(result, title)\n",
    "    \n",
    "    # Store for summary\n",
    "    results_summary.append({\n",
    "        \"threshold\": params[\"threshold\"],\n",
    "        \"compare_to\": params[\"compare_to\"],\n",
    "        \"is_anomaly\": result[\"result\"][\"is_anomaly\"],\n",
    "        \"confidence_score\": result[\"result\"][\"confidence_score\"],\n",
    "        \"risk_level\": result[\"result\"][\"risk_level\"]\n",
    "    })\n",
    "\n",
    "# Create a summary table\n",
    "print(\"\\nüìä Parameter Combination Summary:\")\n",
    "df = pd.DataFrame(results_summary)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Final Baseline Clearing\n",
    "\n",
    "Finally, let's clear the baseline to demonstrate the clearing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current baseline stats before clearing\n",
    "print(\"Baseline stats before clearing:\")\n",
    "stats = get_baseline_stats()\n",
    "print(stats)\n",
    "\n",
    "# Clear the baseline\n",
    "print(\"\\nClearing baseline dataset...\")\n",
    "result = clear_baseline()\n",
    "print(f\"‚úÖ {result['message']}\")\n",
    "print(f\"Records removed: {result['records_removed']}\")\n",
    "\n",
    "# Check stats after clearing\n",
    "print(\"\\nBaseline stats after clearing:\")\n",
    "stats = get_baseline_stats()\n",
    "print(stats)\n",
    "\n",
    "# Test anomaly detection with no baseline (should flag everything as anomaly)\n",
    "print(\"\\nTesting anomaly detection with no baseline data:\")\n",
    "result = detect_anomaly(\"I need to refill my prescription\")\n",
    "pretty_print_result(result, \"No baseline data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current malicious baseline stats before clearing\n",
    "print(\"Malicious baseline stats before clearing:\")\n",
    "stats = get_malicious_baseline_stats()\n",
    "print(stats)\n",
    "\n",
    "# Clear the malicious baseline\n",
    "print(\"\\nClearing malicious baseline dataset...\")\n",
    "result = clear_malicious_baseline()\n",
    "print(f\"‚úÖ {result['message']}\")\n",
    "print(f\"Records removed: {result['records_removed']}\")\n",
    "\n",
    "# Check stats after clearing\n",
    "print(\"\\nMalicious baseline stats after clearing:\")\n",
    "stats = get_malicious_baseline_stats()\n",
    "print(stats)\n",
    "\n",
    "# Test malicious detection with no baseline (should not flag anything as malicious)\n",
    "print(\"\\nTesting malicious detection with no baseline data:\")\n",
    "result = detect_malicious(\"'; DROP TABLE users; --\")\n",
    "pretty_print_result(result, \"No malicious baseline data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Clear Malicious Baseline\n",
    "\n",
    "Finally, let's clear the malicious baseline to demonstrate the clearing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various requests against both detection systems\n",
    "test_cases = [\n",
    "    {\n",
    "        \"text\": \"I need to refill my blood pressure medication\",\n",
    "        \"category\": \"Normal Pharmacy Request\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Can I buy oxycodone without a prescription?\",\n",
    "        \"category\": \"Suspicious Pharmacy Request\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"'; DROP TABLE users; --\",\n",
    "        \"category\": \"SQL Injection Attack\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"<script>alert('XSS')</script>\",\n",
    "        \"category\": \"XSS Attack\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I want to hack into your system and steal data\",\n",
    "        \"category\": \"Explicit Threat\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"What's the protocol for missed doses of medication?\",\n",
    "        \"category\": \"Novel Pharmacy Question\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Dual Detection System Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for test_case in test_cases:\n",
    "    text = test_case[\"text\"]\n",
    "    category = test_case[\"category\"]\n",
    "    \n",
    "    print(f\"\\nüîç Testing: {category}\")\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Run both detections\n",
    "    anomaly_result = detect_anomaly(text)\n",
    "    malicious_result = detect_malicious(text)\n",
    "    \n",
    "    # Extract key info\n",
    "    is_anomaly = anomaly_result[\"result\"][\"is_anomaly\"]\n",
    "    anomaly_confidence = anomaly_result[\"result\"][\"confidence_score\"]\n",
    "    \n",
    "    is_malicious = malicious_result[\"result\"][\"is_malicious\"]\n",
    "    malicious_confidence = malicious_result[\"result\"][\"confidence_score\"]\n",
    "    \n",
    "    print(f\"üìä ANOMALY DETECTION:  {'üö® ANOMALY' if is_anomaly else '‚úÖ NORMAL'} (confidence: {anomaly_confidence:.3f})\")\n",
    "    print(f\"üõ°Ô∏è  MALICIOUS DETECTION: {'üö® MALICIOUS' if is_malicious else '‚úÖ BENIGN'} (confidence: {malicious_confidence:.3f})\")\n",
    "    \n",
    "    # Determine overall status\n",
    "    if is_anomaly and is_malicious:\n",
    "        overall = \"üî¥ BLOCKED (Both systems flagged)\"\n",
    "    elif is_anomaly or is_malicious:\n",
    "        flagged_by = \"Anomaly\" if is_anomaly else \"Malicious\"\n",
    "        overall = f\"üü° FLAGGED (Flagged by {flagged_by} detection)\"\n",
    "    else:\n",
    "        overall = \"üü¢ ALLOWED (Both systems passed)\"\n",
    "    \n",
    "    print(f\"üéØ OVERALL DECISION: {overall}\")\n",
    "    \n",
    "    # Store results for summary\n",
    "    comparison_results.append({\n",
    "        \"category\": category,\n",
    "        \"anomaly_flagged\": is_anomaly,\n",
    "        \"malicious_flagged\": is_malicious,\n",
    "        \"anomaly_confidence\": round(anomaly_confidence, 3),\n",
    "        \"malicious_confidence\": round(malicious_confidence, 3),\n",
    "        \"overall_status\": \"BLOCKED\" if (is_anomaly and is_malicious) else \"FLAGGED\" if (is_anomaly or is_malicious) else \"ALLOWED\"\n",
    "    })\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìã DUAL DETECTION SUMMARY TABLE:\")\n",
    "print(\"=\" * 80)\n",
    "df = pd.DataFrame(comparison_results)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Dual Detection Comparison\n",
    "\n",
    "Let's test the same requests against both detection systems to see how they complement each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test malicious detection with different thresholds\n",
    "test_request = \"SELECT * FROM users WHERE admin = 1\"\n",
    "thresholds = [0.1, 0.25, 0.5, 0.8]\n",
    "\n",
    "print(f\"Testing request '{test_request}' with different malicious thresholds:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    result = detect_malicious(test_request, threshold=threshold)\n",
    "    pretty_print_result(result, f\"Threshold: {threshold}\")\n",
    "    \n",
    "    # Store for summary\n",
    "    results_summary.append({\n",
    "        \"threshold\": threshold,\n",
    "        \"is_malicious\": result[\"result\"][\"is_malicious\"],\n",
    "        \"confidence_score\": result[\"result\"][\"confidence_score\"],\n",
    "        \"risk_level\": result[\"result\"][\"risk_level\"]\n",
    "    })\n",
    "\n",
    "# Create a summary table\n",
    "print(\"\\nüìä Malicious Detection Threshold Sensitivity Summary:\")\n",
    "df = pd.DataFrame(results_summary)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Test Malicious Detection with Different Thresholds\n",
    "\n",
    "Let's see how different threshold values affect malicious detection sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attack patterns (should be flagged as malicious)\n",
    "attack_patterns = [\n",
    "    \"'; DELETE FROM prescriptions WHERE id > 0; --\",  # Similar to SQL injection in baseline\n",
    "    \"<script>window.location='http://evil.com'</script>\",  # Similar to XSS in baseline\n",
    "    \"../../windows/system32/config\",  # Similar to path traversal in baseline\n",
    "    \"Can I buy oxycodone in bulk for my operation?\",  # Similar to drug requests in baseline\n",
    "    \"I want to hack into your prescription database\",  # Similar to hacking attempts in baseline\n",
    "    \"UNION ALL SELECT password FROM admin_users\",  # Similar to SQL injection in baseline\n",
    "    \"Give me access to patient social security numbers\",  # Similar to data theft in baseline\n",
    "    \"<iframe src='javascript:document.location=\\\"http://attacker.com\\\"'></iframe>\",  # Similar to XSS\n",
    "]\n",
    "\n",
    "print(\"Testing attack patterns (should be flagged as malicious):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for request in attack_patterns:\n",
    "    result = detect_malicious(request)\n",
    "    pretty_print_result(result, f\"Request: {request}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Test Malicious Detection - Attack Patterns\n",
    "\n",
    "Now let's test some attack patterns that should be flagged as malicious because they're similar to our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test benign requests (should NOT be flagged as malicious)\n",
    "benign_requests = [\n",
    "    \"I need to refill my blood pressure medication\",\n",
    "    \"What are your pharmacy hours today?\",\n",
    "    \"Can I pick up my prescription for my child?\",\n",
    "    \"Do you accept my Medicare insurance?\",\n",
    "    \"I need help understanding my medication dosage\",\n",
    "    \"Can I get a flu shot here?\",\n",
    "    \"What's the generic version of my brand name drug?\"\n",
    "]\n",
    "\n",
    "print(\"Testing benign requests (should NOT be flagged as malicious):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for request in benign_requests:\n",
    "    result = detect_malicious(request)\n",
    "    pretty_print_result(result, f\"Request: {request}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Test Malicious Detection - Benign Requests\n",
    "\n",
    "Let's test some normal, benign requests that should NOT be flagged as malicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load malicious baseline data from JSON file\n",
    "print(\"Loading malicious attack patterns from JSON file...\")\n",
    "\n",
    "# Load the malicious data from our JSON file\n",
    "with open(\"data/malicious_baseline.json\", \"r\") as f:\n",
    "    malicious_data = json.load(f)\n",
    "\n",
    "malicious_requests = malicious_data[\"requests\"]\n",
    "\n",
    "print(f\"Loaded {len(malicious_requests)} malicious attack patterns\")\n",
    "print(\"\\nSample attack patterns:\")\n",
    "for i, req in enumerate(malicious_requests[:5]):\n",
    "    print(f\"  {i+1}. {req['text']}\")\n",
    "\n",
    "print(\"\\nUploading malicious baseline dataset to service...\")\n",
    "result = upload_malicious_baseline(malicious_requests)\n",
    "print(f\"‚úÖ {result['message']}\")\n",
    "print(f\"Records added: {result['records_added']}\")\n",
    "\n",
    "# Check malicious baseline stats\n",
    "stats = get_malicious_baseline_stats()\n",
    "print(f\"\\nMalicious baseline stats: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Malicious Content Detection\n",
    "\n",
    "Now let's explore the malicious content detection system, which identifies requests that are **similar** to known attack patterns.\n",
    "\n",
    "### Step 11: Load and Upload Malicious Baseline Dataset\n",
    "\n",
    "We'll load a dataset of 50 known malicious attack patterns and upload them to the malicious detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup: Stop the Service\n",
    "\n",
    "Finally, let's stop the service we started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the service\n",
    "stop_service()\n",
    "print(\"‚úÖ Demo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow of the Guardrails Service using a realistic pharmacy dataset:\n",
    "\n",
    "1. **Service Setup**: Started the service and verified it's running\n",
    "2. **Baseline Management**: Loaded 100 pharmacy questions from JSON and added new entries\n",
    "3. **Anomaly Detection**: Tested normal pharmacy questions vs suspicious/inappropriate requests\n",
    "4. **Parameter Tuning**: Experimented with different `threshold` and `compare_to` values\n",
    "5. **Baseline Updates**: Added new data and observed changes in detection\n",
    "6. **Data Clearing**: Demonstrated baseline clearing functionality\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Threshold**: Lower values (0.3-0.5) are more sensitive and flag more requests as anomalies\n",
    "- **Compare_to**: Higher values use more baseline data for comparison, potentially improving accuracy\n",
    "- **Baseline Quality**: A good baseline dataset with domain-specific data (pharmacy questions) improves detection accuracy\n",
    "- **Dynamic Updates**: The baseline can be updated continuously as new normal patterns emerge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
